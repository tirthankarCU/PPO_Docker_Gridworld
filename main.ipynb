{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('~/Desktop/ENV_AGENT1')\n",
    "from simple_gridworld import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy():\n",
    "    return np.random.randint(0,4) \n",
    "\n",
    "env=SimpleGridWorld()\n",
    "episodes=1\n",
    "for _ in range(episodes):\n",
    "    obs=env.reset()\n",
    "    cumulative_reward=0\n",
    "    while True:\n",
    "        obs,reward,trunc=env.action_step(policy())\n",
    "        cumulative_reward+=reward\n",
    "        if trunc:\n",
    "            break \n",
    "    print(f'Cumulative Reward {cumulative_reward}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH=\"/home/trithankar-mittra/Desktop/Env_Agent1\"\n",
    "!rm -rf \"{PROJECT_PATH}/save_state\"\n",
    "!mkdir \"{PROJECT_PATH}/save_state\"\n",
    "!mkdir \"{PROJECT_PATH}/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_timeSteps=25 # agent's expectation of how long each episode will last.\n",
    "env = SimpleGridWorld()\n",
    "num_actions=4\n",
    "update_after_actions=2**6\n",
    "update_target_network=2**8\n",
    "frame_cnt=0\n",
    "episode_reward_hist,episode_avg_reward=[],[]\n",
    "epsilon,epsilon_min,epsilon_max=1.0,0.005,1.0 \n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ") \n",
    "\n",
    "# Expected number of frames \n",
    "episodes=10000\n",
    "total_frames=mx_timeSteps*episodes\n",
    "# New logic.\n",
    "epsilon_greedy=int(0.2*total_frames)\n",
    "epsilon_greedy_frames=int((50/95)*total_frames) # After 50% of total frames I want the value of epsilon to reduce to 5/100 from 1.\n",
    "\n",
    "batch_size=2**10\n",
    "action=-1\n",
    "replay_mem={'state':[],'next_state':[],'action':[],'reward':[]}\n",
    "mx_replay_memory=100000\n",
    "gamma=0.9\n",
    "# MODEL\n",
    "save_step=2**10;load_from_checkpoint=False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs=10\n",
    "first=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=10000\n",
    "load_from_checkpoint=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=M.NNModel().to(device)\n",
    "model_target=M.NNModel().to(device)\n",
    "for param in model.parameters():\n",
    "    param.data.fill_(0)\n",
    "for param in model_target.parameters():\n",
    "    param.data.fill_(0)\n",
    "if load_from_checkpoint==True:\n",
    "    model.load_state_dict(torch.load('results/model.ml'))\n",
    "    model_target.load_state_dict(torch.load('results/model.ml'))\n",
    "    episode_reward_hist=np.load(f'results/ep_tot.npy').tolist()\n",
    "    episode_avg_reward=np.load(f'results/ep_avg.npy').tolist()\n",
    "    replay_mem['state']=np.load(f'results/state.npy').tolist()\n",
    "    replay_mem['next_state']=np.load(f'results/next_state.npy').tolist()\n",
    "    replay_mem['action']=np.load(f'results/action.npy').tolist()\n",
    "    replay_mem['reward']=np.load(f'results/reward.npy').tolist()\n",
    "    file_read=open('results/frame_cnt.dat','r')\n",
    "    frame_cnt=int(file_read.read())\n",
    "    file_read.close()\n",
    "    file_read=open('results/epsilon.dat','r')\n",
    "    epsilon=float(file_read.read())\n",
    "    file_read.close()\n",
    "if first:   \n",
    "    load_from_checkpoint=True\n",
    "    first=False\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "for _ in range(episodes):\n",
    "    episode_reward=0\n",
    "    state = env.reset()\n",
    "    for __ in range(1,mx_timeSteps):\n",
    "        frame_cnt+=1\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        replay_mem['state'].append(state)\n",
    "        if frame_cnt<=epsilon_greedy or epsilon>np.random.rand(1)[0]:\n",
    "            action=np.random.choice(num_actions)\n",
    "        else:\n",
    "            action=torch.argmax(M.predict(model,[state],device))\n",
    "            action=action.item()\n",
    "        next_state,reward,terminated=env.action_step(action)\n",
    "        episode_reward+=reward \n",
    "        state=copy.deepcopy(next_state)\n",
    "        replay_mem['next_state'].append(next_state)\n",
    "        replay_mem['action'].append(action)\n",
    "        replay_mem['reward'].append(reward)\n",
    "        if (frame_cnt%save_step==0):\n",
    "            print(f'Saving ...')\n",
    "            np.save(f'results/ep_tot.npy',np.array(episode_reward_hist))\n",
    "            np.save(f'results/ep_avg.npy',np.array(episode_avg_reward))\n",
    "            np.save(f'results/state.npy',np.array(replay_mem['state']))\n",
    "            np.save(f'results/next_state.npy',np.array(replay_mem['next_state']))\n",
    "            np.save(f'results/action.npy',np.array(replay_mem['action']))\n",
    "            np.save(f'results/reward.npy',np.array(replay_mem['reward']))\n",
    "            file_write=open('results/frame_cnt.dat','w')\n",
    "            file_write.write(str(frame_cnt))\n",
    "            file_write.close()\n",
    "            file_write=open('results/epsilon.dat','w')\n",
    "            file_write.write(str(epsilon))\n",
    "            file_write.close()\n",
    "            torch.save(model.state_dict(),'results/model.ml')\n",
    "        if frame_cnt%update_after_actions==0 and len(replay_mem['reward'])>batch_size:\n",
    "            choices=np.random.choice( range( len(replay_mem['reward']) ), size=batch_size )\n",
    "            STATE=[replay_mem['state'][id] for id in choices]\n",
    "            NEXT_STATE=[replay_mem['next_state'][id] for id in choices]\n",
    "            ACTION=[replay_mem['action'][id] for id in choices]\n",
    "            REWARD=np.array([replay_mem['reward'][id] for id in choices])\n",
    "            with torch.no_grad():\n",
    "                a=M.predict(model_target,NEXT_STATE,device)\n",
    "                TEMP,TEMP_INDX=torch.max(M.predict(model_target,NEXT_STATE,device),dim=1)\n",
    "                REWARD_T=torch.tensor(REWARD)\n",
    "                if torch.cuda.is_available():\n",
    "                    REWARD_T=REWARD_T.to(torch.device(\"cuda:0\")) \n",
    "                reward_true=torch.add(REWARD_T,gamma*TEMP)\n",
    "            for epoch in range(epochs):\n",
    "                M.train(model,reward_true,STATE,ACTION,device,optimizer)\n",
    "        if frame_cnt%update_target_network==0:\n",
    "            print(f'Updating N/W ...{frame_cnt}')\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "        if len(replay_mem['reward'])>mx_replay_memory:\n",
    "            print('Deleting ...')\n",
    "            del replay_mem['state'][:1]\n",
    "            del replay_mem['next_state'][:1]\n",
    "            del replay_mem['action'][:1]\n",
    "            del replay_mem['reward'][:1]\n",
    "        if terminated:\n",
    "            break\n",
    "    episode_reward_hist.append(episode_reward)\n",
    "    episode_avg_reward.append(episode_reward/__)\n",
    "\n",
    "# Save Relevant Results\n",
    "np.save('results/ep_tot.npy',np.array(episode_reward_hist))\n",
    "np.save('results/ep_avg.npy',np.array(episode_avg_reward))\n",
    "torch.save(model.state_dict(),'results/model.ml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Results.\n",
    "episode_reward_hist=np.load('results/ep_tot.npy')\n",
    "episode_avg_reward=np.load('results/ep_avg.npy')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_tr=M.NNModel().to(device)\n",
    "model_tr.load_state_dict(torch.load('results/model.ml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(arr,xlabel,ylabel,title):\n",
    "    plt.plot(arr)\n",
    "    plt.xlabel(f'{xlabel}')\n",
    "    plt.ylabel(f'{ylabel}')\n",
    "    plt.title(f'{title}')\n",
    "    plt.savefig(f'results/{title}.png')\n",
    "    plt.show()\n",
    "    print()\n",
    "\n",
    "plotter(episode_reward_hist,'Episodes','Reward','Reward Per Episode VS Episodes')\n",
    "plotter(episode_avg_reward,'Episodes','Avg Reward','Avg Reward Per Episode VS Episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(observation,device):\n",
    "    with torch.no_grad():\n",
    "        action=torch.argmax(M.predict(model_tr,[observation],device))\n",
    "        return action.item()\n",
    "\n",
    "dbg=True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "env = SimpleGridWorld()\n",
    "episodes=1\n",
    "for _ in range(episodes):\n",
    "    cumulative_reward,steps=0,0\n",
    "    observation = env.reset()\n",
    "    cnt=0\n",
    "    while True:\n",
    "        action = policy(copy.deepcopy(observation),device)  # User-defined policy function\n",
    "        if dbg==True:\n",
    "            env.printGrid()\n",
    "            print(f'action {action}')\n",
    "        observation, reward, terminated = env.action_step(action)\n",
    "        cumulative_reward+=reward\n",
    "        steps+=1\n",
    "        if terminated:\n",
    "            break\n",
    "    print(f'Cumulative Reward ~ {cumulative_reward}; TimeTaken ~ {steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
